{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sherlock2nd_try.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOVGYl2WJ1ot/iU03AYrZPb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/priyadharshini13/data_science/blob/master/Sherlock2nd_try.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nas-GVkHRhO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from ast import literal_eval\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import f1_score\n",
        "sys.path.append(\"..\")"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQx403FPK9u-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "outputId": "b1aab394-b344-4b8f-eb6c-66ceb7c8f767"
      },
      "source": [
        "!unzip src"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  src.zip\n",
            "replace src/deploy/classes_retrain_minimal_sample.npy? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "  inflating: src/deploy/classes_retrain_minimal_sample.npy  \n",
            "  inflating: src/deploy/classes_sherlock.npy  \n",
            "  inflating: src/deploy/predict_sherlock.py  \n",
            "  inflating: src/deploy/train_sherlock.py  \n",
            "  inflating: src/features/bag_of_characters.py  \n",
            "  inflating: src/features/bag_of_words.py  \n",
            "  inflating: src/features/build_features.py  \n",
            "  inflating: src/features/feature_column_identifiers/char_col.tsv  \n",
            "  inflating: src/features/feature_column_identifiers/par_col.tsv  \n",
            "  inflating: src/features/feature_column_identifiers/rest_col.tsv  \n",
            "  inflating: src/features/feature_column_identifiers/word_col.tsv  \n",
            "  inflating: src/features/par_vec_trained_400.pkl  \n",
            "  inflating: src/features/paragraph_vectors.py  \n",
            "  inflating: src/features/word_embeddings.py  \n",
            "  inflating: src/models/retrain_minimal_sample_model.json  \n",
            "  inflating: src/models/sherlock_model.json  \n",
            "  inflating: src/models/sherlock_weights.h5  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ryhqvB3LBoh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from src.features.build_features import build_features\n",
        "from src.features.build_features import _get_data\n",
        "from src.deploy.train_sherlock import train_sherlock\n",
        "from src.deploy.predict_sherlock import predict_sherlock\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fbp0hZHB8shB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from src.deploy.predict_sherlock import _prepare_sherlock_model\n",
        "from src.deploy.predict_sherlock import _prepare_feature_cols\n",
        "from src.deploy.predict_sherlock import _transform_predictions_to_classes"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sgSNTo4Vo4ia",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFa6rmlhxgrJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "f35ee1a5-ae9f-4f7b-a594-663fc82d7ff2"
      },
      "source": [
        "_get_data()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading the raw and preprocessed data into ../data/data.zip.\n",
            "Data was downloaded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pr4CamP20ucY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_test_preprocessed = pd.read_parquet('/data/data/processed/X_test.parquet')\n",
        "y_test_preprocessed = pd.read_parquet('/data/data/processed/y_test.parquet').reset_index(drop=True)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqkSN4JR01fi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "outputId": "12e04e77-6281-4cbd-dffa-a136453b73d7"
      },
      "source": [
        "X_test_preprocessed.head()\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>col_entropy</th>\n",
              "      <th>frac_unique</th>\n",
              "      <th>frac_numcells</th>\n",
              "      <th>frac_textcells</th>\n",
              "      <th>avg_num_cells</th>\n",
              "      <th>std_num_cells</th>\n",
              "      <th>avg_text_cells</th>\n",
              "      <th>std_text_cells</th>\n",
              "      <th>avg_spec_cells</th>\n",
              "      <th>std_spec_cells</th>\n",
              "      <th>avg_word_cells</th>\n",
              "      <th>std_word_cells</th>\n",
              "      <th>word_embedding_avg_0</th>\n",
              "      <th>word_embedding_avg_1</th>\n",
              "      <th>word_embedding_avg_2</th>\n",
              "      <th>word_embedding_avg_3</th>\n",
              "      <th>word_embedding_avg_4</th>\n",
              "      <th>word_embedding_avg_5</th>\n",
              "      <th>word_embedding_avg_6</th>\n",
              "      <th>word_embedding_avg_7</th>\n",
              "      <th>word_embedding_avg_8</th>\n",
              "      <th>word_embedding_avg_9</th>\n",
              "      <th>word_embedding_avg_10</th>\n",
              "      <th>word_embedding_avg_11</th>\n",
              "      <th>word_embedding_avg_12</th>\n",
              "      <th>word_embedding_avg_13</th>\n",
              "      <th>word_embedding_avg_14</th>\n",
              "      <th>word_embedding_avg_15</th>\n",
              "      <th>word_embedding_avg_16</th>\n",
              "      <th>word_embedding_avg_17</th>\n",
              "      <th>word_embedding_avg_18</th>\n",
              "      <th>word_embedding_avg_19</th>\n",
              "      <th>word_embedding_avg_20</th>\n",
              "      <th>word_embedding_avg_21</th>\n",
              "      <th>word_embedding_avg_22</th>\n",
              "      <th>word_embedding_avg_23</th>\n",
              "      <th>word_embedding_avg_24</th>\n",
              "      <th>word_embedding_avg_25</th>\n",
              "      <th>word_embedding_avg_26</th>\n",
              "      <th>word_embedding_avg_27</th>\n",
              "      <th>...</th>\n",
              "      <th>par_vec_360</th>\n",
              "      <th>par_vec_361</th>\n",
              "      <th>par_vec_362</th>\n",
              "      <th>par_vec_363</th>\n",
              "      <th>par_vec_364</th>\n",
              "      <th>par_vec_365</th>\n",
              "      <th>par_vec_366</th>\n",
              "      <th>par_vec_367</th>\n",
              "      <th>par_vec_368</th>\n",
              "      <th>par_vec_369</th>\n",
              "      <th>par_vec_370</th>\n",
              "      <th>par_vec_371</th>\n",
              "      <th>par_vec_372</th>\n",
              "      <th>par_vec_373</th>\n",
              "      <th>par_vec_374</th>\n",
              "      <th>par_vec_375</th>\n",
              "      <th>par_vec_376</th>\n",
              "      <th>par_vec_377</th>\n",
              "      <th>par_vec_378</th>\n",
              "      <th>par_vec_379</th>\n",
              "      <th>par_vec_380</th>\n",
              "      <th>par_vec_381</th>\n",
              "      <th>par_vec_382</th>\n",
              "      <th>par_vec_383</th>\n",
              "      <th>par_vec_384</th>\n",
              "      <th>par_vec_385</th>\n",
              "      <th>par_vec_386</th>\n",
              "      <th>par_vec_387</th>\n",
              "      <th>par_vec_388</th>\n",
              "      <th>par_vec_389</th>\n",
              "      <th>par_vec_390</th>\n",
              "      <th>par_vec_391</th>\n",
              "      <th>par_vec_392</th>\n",
              "      <th>par_vec_393</th>\n",
              "      <th>par_vec_394</th>\n",
              "      <th>par_vec_395</th>\n",
              "      <th>par_vec_396</th>\n",
              "      <th>par_vec_397</th>\n",
              "      <th>par_vec_398</th>\n",
              "      <th>par_vec_399</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2.122181</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>12.290</td>\n",
              "      <td>5.077194</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.718</td>\n",
              "      <td>0.885706</td>\n",
              "      <td>-0.656811</td>\n",
              "      <td>-0.083938</td>\n",
              "      <td>0.118053</td>\n",
              "      <td>-0.104459</td>\n",
              "      <td>-0.096726</td>\n",
              "      <td>-0.092821</td>\n",
              "      <td>-0.304606</td>\n",
              "      <td>0.045336</td>\n",
              "      <td>0.034394</td>\n",
              "      <td>-0.698379</td>\n",
              "      <td>0.215158</td>\n",
              "      <td>-0.187603</td>\n",
              "      <td>0.308728</td>\n",
              "      <td>-0.292527</td>\n",
              "      <td>-0.295568</td>\n",
              "      <td>-0.028309</td>\n",
              "      <td>-0.118570</td>\n",
              "      <td>0.598001</td>\n",
              "      <td>-0.048694</td>\n",
              "      <td>0.193136</td>\n",
              "      <td>-0.424510</td>\n",
              "      <td>-0.282988</td>\n",
              "      <td>0.138600</td>\n",
              "      <td>0.475387</td>\n",
              "      <td>-0.487746</td>\n",
              "      <td>-0.882237</td>\n",
              "      <td>0.115854</td>\n",
              "      <td>-0.163639</td>\n",
              "      <td>...</td>\n",
              "      <td>0.018963</td>\n",
              "      <td>0.034759</td>\n",
              "      <td>-0.091708</td>\n",
              "      <td>-0.052776</td>\n",
              "      <td>0.053528</td>\n",
              "      <td>-0.009031</td>\n",
              "      <td>-0.084832</td>\n",
              "      <td>0.008390</td>\n",
              "      <td>0.016916</td>\n",
              "      <td>-0.006733</td>\n",
              "      <td>-0.108808</td>\n",
              "      <td>0.043290</td>\n",
              "      <td>0.011227</td>\n",
              "      <td>-0.005947</td>\n",
              "      <td>-0.109579</td>\n",
              "      <td>-0.025731</td>\n",
              "      <td>-0.046924</td>\n",
              "      <td>-0.077872</td>\n",
              "      <td>0.038479</td>\n",
              "      <td>-0.140773</td>\n",
              "      <td>0.070412</td>\n",
              "      <td>-0.007635</td>\n",
              "      <td>-0.026769</td>\n",
              "      <td>-0.014825</td>\n",
              "      <td>0.127222</td>\n",
              "      <td>-0.001129</td>\n",
              "      <td>0.027856</td>\n",
              "      <td>0.050030</td>\n",
              "      <td>0.070716</td>\n",
              "      <td>0.067966</td>\n",
              "      <td>0.023563</td>\n",
              "      <td>-0.029472</td>\n",
              "      <td>0.002835</td>\n",
              "      <td>0.090851</td>\n",
              "      <td>-0.125505</td>\n",
              "      <td>-0.027747</td>\n",
              "      <td>0.028412</td>\n",
              "      <td>-0.078901</td>\n",
              "      <td>0.054292</td>\n",
              "      <td>-0.049115</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3.817487</td>\n",
              "      <td>0.015</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.058</td>\n",
              "      <td>0.233743</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.256089</td>\n",
              "      <td>0.450894</td>\n",
              "      <td>1.439564</td>\n",
              "      <td>-0.288024</td>\n",
              "      <td>0.033383</td>\n",
              "      <td>0.077213</td>\n",
              "      <td>-0.249244</td>\n",
              "      <td>-0.618758</td>\n",
              "      <td>-0.026506</td>\n",
              "      <td>0.489126</td>\n",
              "      <td>-0.343285</td>\n",
              "      <td>-0.993315</td>\n",
              "      <td>0.030641</td>\n",
              "      <td>-0.301514</td>\n",
              "      <td>0.208875</td>\n",
              "      <td>-0.671689</td>\n",
              "      <td>-0.295255</td>\n",
              "      <td>0.311381</td>\n",
              "      <td>-1.199069</td>\n",
              "      <td>0.160867</td>\n",
              "      <td>-0.319258</td>\n",
              "      <td>0.505732</td>\n",
              "      <td>0.797457</td>\n",
              "      <td>-0.277344</td>\n",
              "      <td>-0.394915</td>\n",
              "      <td>-0.326457</td>\n",
              "      <td>0.280787</td>\n",
              "      <td>-0.280302</td>\n",
              "      <td>...</td>\n",
              "      <td>0.083358</td>\n",
              "      <td>0.000150</td>\n",
              "      <td>-0.083202</td>\n",
              "      <td>0.003529</td>\n",
              "      <td>0.029953</td>\n",
              "      <td>-0.006308</td>\n",
              "      <td>-0.076494</td>\n",
              "      <td>-0.009189</td>\n",
              "      <td>0.110055</td>\n",
              "      <td>0.012941</td>\n",
              "      <td>-0.036507</td>\n",
              "      <td>0.019940</td>\n",
              "      <td>-0.008527</td>\n",
              "      <td>-0.104649</td>\n",
              "      <td>0.031387</td>\n",
              "      <td>-0.002314</td>\n",
              "      <td>0.017444</td>\n",
              "      <td>0.020669</td>\n",
              "      <td>0.017624</td>\n",
              "      <td>-0.004373</td>\n",
              "      <td>-0.040759</td>\n",
              "      <td>0.023832</td>\n",
              "      <td>0.085738</td>\n",
              "      <td>-0.029784</td>\n",
              "      <td>0.003217</td>\n",
              "      <td>-0.016318</td>\n",
              "      <td>-0.051304</td>\n",
              "      <td>-0.056671</td>\n",
              "      <td>-0.007558</td>\n",
              "      <td>-0.063549</td>\n",
              "      <td>0.244085</td>\n",
              "      <td>-0.055574</td>\n",
              "      <td>0.017600</td>\n",
              "      <td>0.079978</td>\n",
              "      <td>-0.014825</td>\n",
              "      <td>0.006086</td>\n",
              "      <td>0.121871</td>\n",
              "      <td>-0.078689</td>\n",
              "      <td>-0.069111</td>\n",
              "      <td>-0.112550</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3.166061</td>\n",
              "      <td>0.009</td>\n",
              "      <td>0.120</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.120</td>\n",
              "      <td>0.324962</td>\n",
              "      <td>11.527</td>\n",
              "      <td>2.688730</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.120</td>\n",
              "      <td>0.324962</td>\n",
              "      <td>0.241223</td>\n",
              "      <td>0.006051</td>\n",
              "      <td>-0.003913</td>\n",
              "      <td>0.357453</td>\n",
              "      <td>0.419776</td>\n",
              "      <td>-0.125143</td>\n",
              "      <td>-0.551759</td>\n",
              "      <td>0.143251</td>\n",
              "      <td>-0.404179</td>\n",
              "      <td>0.410601</td>\n",
              "      <td>0.424946</td>\n",
              "      <td>0.230606</td>\n",
              "      <td>-0.462737</td>\n",
              "      <td>0.231832</td>\n",
              "      <td>0.209193</td>\n",
              "      <td>-0.853379</td>\n",
              "      <td>-0.285458</td>\n",
              "      <td>-0.287638</td>\n",
              "      <td>-0.050693</td>\n",
              "      <td>0.065135</td>\n",
              "      <td>-0.869810</td>\n",
              "      <td>0.336315</td>\n",
              "      <td>-0.077243</td>\n",
              "      <td>0.016658</td>\n",
              "      <td>-0.008452</td>\n",
              "      <td>-0.298465</td>\n",
              "      <td>0.469741</td>\n",
              "      <td>-0.274875</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.010772</td>\n",
              "      <td>-0.033422</td>\n",
              "      <td>-0.106450</td>\n",
              "      <td>-0.060511</td>\n",
              "      <td>0.040506</td>\n",
              "      <td>0.157334</td>\n",
              "      <td>-0.080773</td>\n",
              "      <td>0.002203</td>\n",
              "      <td>0.080604</td>\n",
              "      <td>-0.007194</td>\n",
              "      <td>0.022550</td>\n",
              "      <td>0.037482</td>\n",
              "      <td>0.019114</td>\n",
              "      <td>0.004688</td>\n",
              "      <td>0.021514</td>\n",
              "      <td>-0.050845</td>\n",
              "      <td>-0.075797</td>\n",
              "      <td>-0.031557</td>\n",
              "      <td>-0.010136</td>\n",
              "      <td>-0.107318</td>\n",
              "      <td>0.079314</td>\n",
              "      <td>0.009531</td>\n",
              "      <td>-0.076809</td>\n",
              "      <td>-0.009571</td>\n",
              "      <td>0.016141</td>\n",
              "      <td>0.053626</td>\n",
              "      <td>0.029289</td>\n",
              "      <td>0.094540</td>\n",
              "      <td>0.094459</td>\n",
              "      <td>-0.029275</td>\n",
              "      <td>0.018266</td>\n",
              "      <td>-0.088117</td>\n",
              "      <td>-0.048036</td>\n",
              "      <td>-0.011286</td>\n",
              "      <td>-0.109643</td>\n",
              "      <td>-0.070223</td>\n",
              "      <td>-0.009666</td>\n",
              "      <td>-0.081991</td>\n",
              "      <td>-0.041528</td>\n",
              "      <td>-0.094458</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2.316887</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>9.053</td>\n",
              "      <td>1.960151</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.107724</td>\n",
              "      <td>-0.103557</td>\n",
              "      <td>-0.317374</td>\n",
              "      <td>-0.696078</td>\n",
              "      <td>0.980390</td>\n",
              "      <td>0.513733</td>\n",
              "      <td>-0.597597</td>\n",
              "      <td>-0.352025</td>\n",
              "      <td>-0.433445</td>\n",
              "      <td>0.213745</td>\n",
              "      <td>0.114791</td>\n",
              "      <td>-0.658426</td>\n",
              "      <td>0.473754</td>\n",
              "      <td>-0.225857</td>\n",
              "      <td>0.358932</td>\n",
              "      <td>0.002211</td>\n",
              "      <td>0.510383</td>\n",
              "      <td>-0.564047</td>\n",
              "      <td>0.162679</td>\n",
              "      <td>0.638926</td>\n",
              "      <td>-0.045223</td>\n",
              "      <td>-0.009321</td>\n",
              "      <td>-0.000032</td>\n",
              "      <td>0.815823</td>\n",
              "      <td>-0.187644</td>\n",
              "      <td>-0.341963</td>\n",
              "      <td>-0.633401</td>\n",
              "      <td>-0.833000</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.001497</td>\n",
              "      <td>0.040882</td>\n",
              "      <td>-0.044141</td>\n",
              "      <td>-0.010476</td>\n",
              "      <td>0.083359</td>\n",
              "      <td>0.055717</td>\n",
              "      <td>-0.004526</td>\n",
              "      <td>-0.108602</td>\n",
              "      <td>0.064535</td>\n",
              "      <td>0.009092</td>\n",
              "      <td>-0.098262</td>\n",
              "      <td>0.049295</td>\n",
              "      <td>0.045007</td>\n",
              "      <td>0.003999</td>\n",
              "      <td>-0.041582</td>\n",
              "      <td>-0.075968</td>\n",
              "      <td>-0.113576</td>\n",
              "      <td>0.028032</td>\n",
              "      <td>-0.055899</td>\n",
              "      <td>-0.089232</td>\n",
              "      <td>-0.037735</td>\n",
              "      <td>0.002619</td>\n",
              "      <td>-0.106776</td>\n",
              "      <td>-0.002728</td>\n",
              "      <td>0.075092</td>\n",
              "      <td>0.043294</td>\n",
              "      <td>-0.014771</td>\n",
              "      <td>0.038083</td>\n",
              "      <td>-0.056276</td>\n",
              "      <td>0.033611</td>\n",
              "      <td>-0.063415</td>\n",
              "      <td>-0.000197</td>\n",
              "      <td>0.012020</td>\n",
              "      <td>-0.033859</td>\n",
              "      <td>0.063092</td>\n",
              "      <td>0.075499</td>\n",
              "      <td>-0.009511</td>\n",
              "      <td>-0.070606</td>\n",
              "      <td>0.061907</td>\n",
              "      <td>0.065065</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6.955528</td>\n",
              "      <td>0.163</td>\n",
              "      <td>0.018</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.072</td>\n",
              "      <td>0.531804</td>\n",
              "      <td>20.268</td>\n",
              "      <td>9.593132</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.555</td>\n",
              "      <td>1.516237</td>\n",
              "      <td>0.151338</td>\n",
              "      <td>-0.033225</td>\n",
              "      <td>0.389483</td>\n",
              "      <td>0.962521</td>\n",
              "      <td>-0.119877</td>\n",
              "      <td>0.078157</td>\n",
              "      <td>-0.863531</td>\n",
              "      <td>-0.674138</td>\n",
              "      <td>0.405948</td>\n",
              "      <td>-0.058860</td>\n",
              "      <td>0.403702</td>\n",
              "      <td>0.478408</td>\n",
              "      <td>-0.421121</td>\n",
              "      <td>-0.338476</td>\n",
              "      <td>-0.456249</td>\n",
              "      <td>0.346219</td>\n",
              "      <td>-0.346498</td>\n",
              "      <td>0.260789</td>\n",
              "      <td>-0.308427</td>\n",
              "      <td>-0.377457</td>\n",
              "      <td>0.989781</td>\n",
              "      <td>0.104424</td>\n",
              "      <td>-0.239668</td>\n",
              "      <td>-0.278157</td>\n",
              "      <td>-0.597333</td>\n",
              "      <td>-0.593023</td>\n",
              "      <td>-0.306190</td>\n",
              "      <td>-0.512784</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.035822</td>\n",
              "      <td>0.171019</td>\n",
              "      <td>-0.026016</td>\n",
              "      <td>0.116603</td>\n",
              "      <td>0.016681</td>\n",
              "      <td>-0.019607</td>\n",
              "      <td>-0.081483</td>\n",
              "      <td>0.139963</td>\n",
              "      <td>0.052561</td>\n",
              "      <td>0.062741</td>\n",
              "      <td>-0.111332</td>\n",
              "      <td>0.048563</td>\n",
              "      <td>-0.071029</td>\n",
              "      <td>0.103916</td>\n",
              "      <td>-0.198125</td>\n",
              "      <td>-0.037844</td>\n",
              "      <td>-0.120097</td>\n",
              "      <td>0.051985</td>\n",
              "      <td>0.034595</td>\n",
              "      <td>-0.059780</td>\n",
              "      <td>0.113690</td>\n",
              "      <td>0.090615</td>\n",
              "      <td>0.054712</td>\n",
              "      <td>0.078268</td>\n",
              "      <td>0.065793</td>\n",
              "      <td>0.126138</td>\n",
              "      <td>0.003292</td>\n",
              "      <td>-0.025928</td>\n",
              "      <td>0.071305</td>\n",
              "      <td>0.040046</td>\n",
              "      <td>0.015399</td>\n",
              "      <td>-0.213604</td>\n",
              "      <td>0.029100</td>\n",
              "      <td>-0.009626</td>\n",
              "      <td>-0.154028</td>\n",
              "      <td>-0.090470</td>\n",
              "      <td>-0.013950</td>\n",
              "      <td>0.036592</td>\n",
              "      <td>-0.139673</td>\n",
              "      <td>-0.115430</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 1588 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   col_entropy  frac_unique  ...  par_vec_398  par_vec_399\n",
              "0     2.122181        0.005  ...     0.054292    -0.049115\n",
              "1     3.817487        0.015  ...    -0.069111    -0.112550\n",
              "2     3.166061        0.009  ...    -0.041528    -0.094458\n",
              "3     2.316887        0.005  ...     0.061907     0.065065\n",
              "4     6.955528        0.163  ...    -0.139673    -0.115430\n",
              "\n",
              "[5 rows x 1588 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10xB5hOV07Km",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "outputId": "e264cd9b-4ee9-421c-8893-8156d525cc0f"
      },
      "source": [
        "y_test_preprocessed.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>affiliation</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>weight</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>jockey</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>religion</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>company</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         label\n",
              "0  affiliation\n",
              "1       weight\n",
              "2       jockey\n",
              "3     religion\n",
              "4      company"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxZp77Gf7X3G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c3d98969-44bf-4d22-f521-b237fac4f21c"
      },
      "source": [
        "type(X_test_preprocessed)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pandas.core.frame.DataFrame"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTU0GEao1ARt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6e0bd933-b4f3-4510-fcd8-f1a4057b5807"
      },
      "source": [
        "predicted_labels = predict_sherlock(X_test_preprocessed, 'sherlock')\n",
        "print('Predicted labels: ', predicted_labels, 'true labels: ', y_test_preprocessed)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-455e984a6d51>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredicted_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_sherlock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_preprocessed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sherlock'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Predicted labels: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'true labels: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_preprocessed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/src/deploy/predict_sherlock.py\u001b[0m in \u001b[0;36mpredict_sherlock\u001b[0;34m(X, nn_id)\u001b[0m\n\u001b[1;32m     49\u001b[0m                                \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature_cols_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'word'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m                                \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature_cols_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'par'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                                X[feature_cols_dict['rest']].values])\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_transform_predictions_to_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m       raise ValueError('{} is not supported in multi-worker mode.'.format(\n\u001b[1;32m    129\u001b[0m           method.__name__))\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m   return tf_decorator.make_decorator(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1577\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1578\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1579\u001b[0;31m           steps_per_execution=self._steps_per_execution)\n\u001b[0m\u001b[1;32m   1580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1581\u001b[0m       \u001b[0;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)\u001b[0m\n\u001b[1;32m   1115\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m         \u001b[0mdistribution_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1117\u001b[0;31m         model=model)\n\u001b[0m\u001b[1;32m   1118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m                **kwargs):\n\u001b[1;32m    264\u001b[0m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensorLikeDataAdapter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m     \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_tensorlike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m     sample_weight_modes = broadcast_sample_weight_modes(\n\u001b[1;32m    267\u001b[0m         sample_weights, sample_weight_modes)\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m_process_tensorlike\u001b[0;34m(inputs)\u001b[0m\n\u001b[1;32m   1019\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1020\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m   \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_convert_numpy_and_scipy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1022\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_to_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 635\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    637\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 635\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    637\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m_convert_numpy_and_scipy\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1014\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0missubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloating\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1015\u001b[0m         \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloatx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1016\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1017\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mscipy_sparse\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mscipy_sparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1018\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_scipy_sparse_to_sparse_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1498\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1499\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1501\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_conversion_registry.py\u001b[0m in \u001b[0;36m_default_conversion_function\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_default_conversion_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mdel\u001b[0m \u001b[0mas_ref\u001b[0m  \u001b[0;31m# Unused.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    262\u001b[0m   \"\"\"\n\u001b[1;32m    263\u001b[0m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0;32m--> 264\u001b[0;31m                         allow_broadcast=True)\n\u001b[0m\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    273\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tf.constant\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m   \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_eager_impl\u001b[0;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m   \u001b[0;34m\"\"\"Implementation of eager constant.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m   \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_eager_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m     96\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type bool)."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "af2VrtWKv9Za",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "72091a3d-6888-4102-98db-78d79f3bcad8"
      },
      "source": [
        "sherlock = _prepare_sherlock_model('sherlock')\n",
        "type(sherlock)\n",
        "print(sherlock)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<tensorflow.python.keras.engine.functional.Functional object at 0x7f5ee765b2b0>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80ltAu7VxFIm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQ6gQlZTxI8D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sherlock.layers\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CD8Oe0uZ1Qba",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(len(sherlock.layers))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "us4zZf1w1bNd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sherlock.name"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmmEJMWT2G4L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sherlock.trainable_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBL3dZWZ-Xex",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sherlock.metrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ROL3MGe4vay",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model = tf.keras.Sequential([\n",
        "# tf.keras.layers.Dense(5, input_shape=(3,)),\n",
        "# tf.keras.layers.Softmax()])\n",
        "# config = model.to_json()\n",
        "# loaded_model = tf.keras.models.model_from_json(config)\n",
        "# config"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMyG0Hw6-0sK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "outputId": "8050bf34-eec6-4204-e14e-e026deef43d2"
      },
      "source": [
        "feature_cols_dict = _prepare_feature_cols()\n",
        "print(len(feature_cols_dict))\n",
        "print(len(feature_cols_dict['char']))\n",
        "print(len(feature_cols_dict['word']))\n",
        "print(len(feature_cols_dict['par']))\n",
        "print(len(feature_cols_dict['rest']))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4\n",
            "960\n",
            "201\n",
            "400\n",
            "27\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDE7P77qBUPS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 994
        },
        "outputId": "3f8e0aef-bbfc-4450-ccec-5cc3940d3e87"
      },
      "source": [
        "sherlock.summary()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 960)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, 201)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_3 (InputLayer)            [(None, 400)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 960)          3840        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 201)          804         input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 400)          1600        input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 300)          288300      batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 200)          40400       batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 400)          160400      batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 300)          0           dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 200)          0           dense_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 400)          0           dense_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "input_4 (InputLayer)            [(None, 27)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 300)          90300       dropout[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 200)          40200       dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 400)          160400      dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 27)           108         input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 927)          0           dense_1[0][0]                    \n",
            "                                                                 dense_3[0][0]                    \n",
            "                                                                 dense_5[0][0]                    \n",
            "                                                                 batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 927)          3708        concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_6 (Dense)                 (None, 500)          464000      batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, 500)          0           dense_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_7 (Dense)                 (None, 500)          250500      dropout_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_8 (Dense)                 (None, 78)           39078       dense_7[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 1,543,638\n",
            "Trainable params: 1,538,608\n",
            "Non-trainable params: 5,030\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvJ-ELX8kGMI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with np.printoptions(threshold=np.inf):\n",
        "    display(np.unique(X[feature_cols_dict['rest']].values))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxkO_XV3kfPg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "197ee84c-dd5d-4cef-f63e-c484793ba4ea"
      },
      "source": [
        "with np.printoptions(threshold=np.inf):\n",
        "    print(type(np.unique(X[feature_cols_dict['rest']].values)))"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nasiRx49lYw7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d1648961-b8a1-4a1e-dc4c-793285ca7f26"
      },
      "source": [
        "len(X[feature_cols_dict['rest']].values)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "137353"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XcBUog0zl1dj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "outputId": "6f506905-819e-4f80-e7b3-60ba25f64f51"
      },
      "source": [
        "X[feature_cols_dict['rest']].values[0]\n",
        "#137353 rows and 27 columns"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2.1221809961106657, 0.005, 0.0, 1.0, 0.0, 0.0, 12.29,\n",
              "       5.077194106984685, 0.0, 0.0, 1.7180000000000002,\n",
              "       0.8857064976616125, 7, True, True, 13.008, 34.457936000000004, 4,\n",
              "       23, 10.0, 13008, -0.9090802409651356, 0.24233320011559165, False,\n",
              "       0.0, 0, False], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVrHScxPmNUW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bfc8f92f-e494-461d-8e0a-2f5052010803"
      },
      "source": [
        "type(X[feature_cols_dict['rest']].values)\n"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08D5wNlNmbC6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "outputId": "189a835d-be3b-45a6-f66c-d17745df6b16"
      },
      "source": [
        "arr = X[feature_cols_dict['rest']].values\n",
        "a = arr.astype(float)\n",
        "a[0]"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 2.12218100e+00,  5.00000000e-03,  0.00000000e+00,  1.00000000e+00,\n",
              "        0.00000000e+00,  0.00000000e+00,  1.22900000e+01,  5.07719411e+00,\n",
              "        0.00000000e+00,  0.00000000e+00,  1.71800000e+00,  8.85706498e-01,\n",
              "        7.00000000e+00,  1.00000000e+00,  1.00000000e+00,  1.30080000e+01,\n",
              "        3.44579360e+01,  4.00000000e+00,  2.30000000e+01,  1.00000000e+01,\n",
              "        1.30080000e+04, -9.09080241e-01,  2.42333200e-01,  0.00000000e+00,\n",
              "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJk4bcGcmynY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "58ccb09e-536f-426f-f62f-69a32a126638"
      },
      "source": [
        "a[0][1]"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.005"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Ra8MHSQmt4m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X[feature_cols_dict['rest']].values\n",
        "astype(float)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4fef_SWmt19",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "555f4867-b2a0-40d0-ff65-75d9f305efe5"
      },
      "source": [
        "# ch = X[feature_cols_dict['char']].values\n",
        "# ch = ch.astype(float)\n",
        "y_pred = sherlock.predict([X[feature_cols_dict['char']].values.astype(float),\n",
        "                               X[feature_cols_dict['word']].values.astype(float),\n",
        "                               X[feature_cols_dict['par']].values.astype(float),\n",
        "                               X[feature_cols_dict['rest']].values.astype(float)])\n",
        "y_pred[0:5]"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[7.15851955e-09, 1.00603738e-05, 9.99130905e-01, 2.89577824e-11,\n",
              "        1.15475762e-08, 1.41885585e-06, 5.69408343e-08, 6.66702638e-10,\n",
              "        1.47264236e-08, 6.74201956e-08, 4.75312678e-10, 1.01320765e-07,\n",
              "        1.84419684e-11, 1.58631615e-08, 6.37802623e-06, 5.80178948e-06,\n",
              "        5.58600805e-11, 3.10967927e-08, 8.27881941e-09, 3.19698813e-07,\n",
              "        8.43444781e-08, 1.25888466e-09, 1.76223995e-08, 4.97836550e-10,\n",
              "        1.73401240e-08, 2.74070857e-07, 1.39805652e-08, 1.00739095e-09,\n",
              "        3.38800134e-11, 4.86834306e-09, 2.91461166e-08, 4.95226221e-11,\n",
              "        8.77919604e-07, 1.25750160e-10, 7.07892722e-09, 7.23905935e-10,\n",
              "        1.89355018e-10, 4.44984760e-09, 4.01780254e-09, 7.24534788e-09,\n",
              "        1.50196911e-07, 4.16931795e-10, 2.68598654e-09, 2.36386573e-08,\n",
              "        2.14308677e-08, 4.22628688e-07, 8.98918415e-08, 5.25205508e-07,\n",
              "        2.61244537e-07, 1.23663835e-06, 3.37417805e-09, 2.05432916e-05,\n",
              "        8.29435692e-07, 7.65557401e-04, 2.55943746e-07, 1.39051115e-09,\n",
              "        3.40965562e-06, 6.63947919e-09, 1.65771132e-06, 1.35259359e-10,\n",
              "        7.37168193e-09, 7.44491802e-09, 2.06121431e-08, 7.66765481e-07,\n",
              "        1.51895364e-07, 1.10862908e-09, 3.11890869e-09, 9.04789715e-07,\n",
              "        1.01578514e-07, 2.28301333e-09, 3.06649994e-08, 6.89563890e-08,\n",
              "        3.85990351e-10, 2.81851226e-06, 4.34740614e-05, 1.46336404e-07,\n",
              "        3.26796312e-10, 2.46158161e-09],\n",
              "       [6.79369332e-05, 2.30021964e-04, 9.23467378e-05, 5.76062165e-02,\n",
              "        3.12938901e-05, 1.28873519e-03, 2.67685373e-05, 2.83556747e-05,\n",
              "        1.41550322e-06, 1.70308958e-05, 3.25724599e-04, 6.94903021e-04,\n",
              "        8.16470929e-05, 3.53440084e-03, 5.79573934e-05, 2.51781748e-05,\n",
              "        2.30394807e-02, 1.02311271e-04, 1.49984640e-04, 7.01764220e-05,\n",
              "        1.84072866e-04, 7.20808021e-05, 1.06373499e-03, 2.14023850e-04,\n",
              "        7.62212358e-06, 6.85442501e-05, 1.32567593e-05, 4.86371457e-04,\n",
              "        1.89871143e-03, 9.32357361e-05, 7.98299370e-05, 1.55587890e-03,\n",
              "        3.49092577e-03, 3.24732100e-04, 1.76033762e-04, 2.02818246e-05,\n",
              "        6.10724092e-05, 8.74533202e-04, 2.00871764e-05, 2.48582975e-04,\n",
              "        2.80211127e-04, 3.41278501e-05, 5.95184065e-06, 2.97613617e-04,\n",
              "        1.67070742e-04, 2.81498637e-06, 1.11498463e-03, 2.03419433e-04,\n",
              "        6.40201833e-05, 3.99135424e-05, 6.19136437e-04, 7.68514001e-05,\n",
              "        2.94516540e-05, 1.53703240e-05, 1.27122781e-04, 1.54889771e-03,\n",
              "        2.28945655e-03, 5.82451932e-04, 6.64090123e-07, 1.77100040e-02,\n",
              "        6.47293553e-02, 2.05681454e-02, 1.73334163e-02, 4.35152266e-04,\n",
              "        2.28302437e-04, 8.97041336e-03, 2.24798525e-04, 4.71378182e-04,\n",
              "        1.63693354e-03, 2.41010377e-04, 7.60456081e-04, 1.91267536e-04,\n",
              "        9.64126521e-05, 3.01172313e-05, 1.25811230e-05, 2.01764142e-05,\n",
              "        7.38319695e-01, 2.20953524e-02],\n",
              "       [5.88024117e-12, 1.83843166e-11, 1.15309515e-11, 9.45337762e-14,\n",
              "        9.26242937e-12, 3.44390358e-12, 6.85070560e-08, 2.04889138e-13,\n",
              "        8.60501788e-14, 1.86498733e-11, 2.89717686e-12, 4.04519571e-13,\n",
              "        1.61222966e-12, 2.63641210e-11, 1.48229541e-12, 9.89166249e-11,\n",
              "        1.27138890e-11, 8.61885324e-13, 2.23975591e-12, 9.33014083e-13,\n",
              "        4.13953984e-12, 1.39327994e-15, 4.69433317e-14, 1.03786684e-12,\n",
              "        4.20262353e-10, 1.67376912e-09, 2.69694011e-12, 5.69649970e-12,\n",
              "        1.17534183e-12, 1.92155059e-12, 4.56154403e-08, 1.45785321e-13,\n",
              "        2.50269894e-14, 2.57131230e-13, 4.73157347e-11, 1.85378297e-12,\n",
              "        1.84442780e-14, 6.22983544e-13, 1.77381981e-14, 1.93543505e-13,\n",
              "        2.22166980e-15, 5.16205324e-14, 9.99999285e-01, 3.02339369e-11,\n",
              "        2.33619710e-13, 1.90854433e-11, 3.41452477e-07, 5.73415230e-12,\n",
              "        1.07500532e-12, 1.94328633e-11, 2.27478071e-11, 3.89863346e-14,\n",
              "        5.16811680e-12, 4.71697170e-09, 2.74043401e-07, 3.73033765e-12,\n",
              "        3.94781294e-13, 1.19944267e-12, 1.51466730e-13, 2.24617842e-11,\n",
              "        4.12851654e-12, 6.79154823e-12, 8.21090700e-13, 2.25249268e-12,\n",
              "        6.37528788e-12, 3.70231901e-11, 6.97326311e-14, 1.22900670e-12,\n",
              "        4.18392794e-12, 1.85595966e-11, 2.81740455e-14, 1.03298629e-13,\n",
              "        9.78409736e-13, 2.33694383e-11, 8.06110213e-12, 2.13324757e-12,\n",
              "        1.43312310e-13, 9.71536653e-13],\n",
              "       [4.20163957e-07, 1.05733770e-05, 6.09787858e-05, 3.37419692e-09,\n",
              "        4.49163480e-07, 5.65473272e-07, 1.12798334e-05, 2.84408441e-10,\n",
              "        1.72760339e-09, 4.66786929e-08, 3.31764088e-10, 1.41250741e-04,\n",
              "        8.16801062e-08, 2.16858211e-06, 4.08743404e-07, 3.76865152e-08,\n",
              "        2.39935218e-07, 3.41016198e-06, 1.14147820e-08, 1.20557235e-08,\n",
              "        1.23282060e-08, 1.56774789e-08, 9.54158804e-06, 1.27771315e-07,\n",
              "        1.20780026e-08, 2.06917372e-09, 1.45689620e-08, 3.93565180e-09,\n",
              "        3.54262703e-10, 1.72087553e-08, 2.16546763e-08, 2.17511564e-09,\n",
              "        1.15757075e-05, 1.21266319e-09, 3.46095039e-05, 4.25739860e-11,\n",
              "        1.55070353e-07, 8.42874158e-07, 1.01840296e-05, 6.11621198e-09,\n",
              "        6.18527883e-06, 9.43589917e-09, 1.23466418e-10, 1.59983279e-06,\n",
              "        1.84682250e-07, 1.54102722e-08, 5.36990058e-07, 2.85236136e-04,\n",
              "        8.40050163e-09, 8.58984990e-08, 2.23205170e-08, 1.37016642e-07,\n",
              "        3.72791037e-05, 3.60188153e-07, 1.39072853e-07, 1.49444324e-09,\n",
              "        6.88604302e-08, 6.86762380e-09, 1.46201273e-06, 3.58046819e-08,\n",
              "        6.54913412e-09, 3.00503644e-09, 3.60958961e-06, 9.99355614e-01,\n",
              "        1.22764021e-07, 3.06070298e-08, 2.26373487e-07, 1.08577947e-08,\n",
              "        4.10407574e-06, 6.20216326e-07, 7.91064338e-07, 7.79298830e-08,\n",
              "        1.46166981e-08, 2.09477278e-07, 1.31844303e-07, 1.00906732e-06,\n",
              "        7.55333129e-10, 7.96624249e-07],\n",
              "       [4.49445594e-07, 7.05280458e-04, 3.37794004e-03, 2.69939839e-08,\n",
              "        1.20957466e-10, 2.77374193e-06, 5.00877118e-09, 1.93418010e-08,\n",
              "        2.90223614e-08, 2.45626055e-04, 1.22725325e-07, 5.20198637e-06,\n",
              "        3.94649504e-08, 7.47764162e-08, 7.00813371e-06, 6.11599305e-09,\n",
              "        2.42236069e-07, 1.16562148e-07, 7.01587695e-08, 9.89059210e-01,\n",
              "        2.20268589e-07, 3.71281118e-07, 6.00078351e-07, 7.67388499e-08,\n",
              "        3.06816958e-07, 5.57056637e-07, 1.62682980e-07, 3.26655453e-07,\n",
              "        1.53428221e-08, 1.91945128e-05, 2.36364758e-06, 1.11477638e-09,\n",
              "        4.73152841e-06, 2.34579578e-09, 7.12930778e-05, 1.62970313e-08,\n",
              "        1.97481920e-09, 1.21018800e-07, 3.04263139e-08, 1.20950034e-07,\n",
              "        1.15162972e-03, 8.22061796e-09, 1.92169858e-09, 9.97843030e-10,\n",
              "        2.24310752e-05, 5.75148093e-04, 4.26375167e-03, 1.50210049e-08,\n",
              "        7.99278837e-07, 7.59785034e-05, 2.75218383e-07, 2.51197907e-05,\n",
              "        2.84087491e-07, 1.27624065e-04, 5.34210594e-05, 9.76996928e-10,\n",
              "        1.05530380e-04, 1.42521105e-06, 3.77099932e-05, 7.09478087e-10,\n",
              "        3.90885248e-07, 3.97074693e-08, 6.61761533e-07, 1.51086937e-07,\n",
              "        1.23353672e-07, 3.16246656e-08, 1.90268509e-06, 3.89569914e-06,\n",
              "        2.30496099e-07, 1.41996077e-06, 1.49602528e-07, 3.18454134e-08,\n",
              "        3.37301426e-05, 2.10633033e-07, 7.93070114e-07, 6.95466997e-06,\n",
              "        5.40298961e-09, 3.23142262e-06]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3KazskSgGcV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = X_test_preprocessed\n",
        "X.head()\n",
        "np.nunique(X[feature_cols_dict['char']].values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0eeF_GToIz6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257
        },
        "outputId": "7832a5e4-2e09-4b5d-8ef0-a99afe707a8a"
      },
      "source": [
        "np.load('src/deploy/classes_sherlock.npy', allow_pickle=True)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['address', 'affiliate', 'affiliation', 'age', 'album', 'area',\n",
              "       'artist', 'birth Date', 'birth Place', 'brand', 'capacity',\n",
              "       'category', 'city', 'class', 'classification', 'club', 'code',\n",
              "       'collection', 'command', 'company', 'component', 'continent',\n",
              "       'country', 'county', 'creator', 'credit', 'currency', 'day',\n",
              "       'depth', 'description', 'director', 'duration', 'education',\n",
              "       'elevation', 'family', 'file Size', 'format', 'gender', 'genre',\n",
              "       'grades', 'industry', 'isbn', 'jockey', 'language', 'location',\n",
              "       'manufacturer', 'name', 'nationality', 'notes', 'operator',\n",
              "       'order', 'organisation', 'origin', 'owner', 'person', 'plays',\n",
              "       'position', 'product', 'publisher', 'range', 'rank', 'ranking',\n",
              "       'region', 'religion', 'requirement', 'result', 'sales', 'service',\n",
              "       'sex', 'species', 'state', 'status', 'symbol', 'team', 'team Name',\n",
              "       'type', 'weight', 'year'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxOvVLwRozkg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "32c087f1-893a-49f7-89c0-2505a87e02b6"
      },
      "source": [
        "output = _transform_predictions_to_classes(y_pred, 'sherlock')\n",
        "output.shape\n",
        "output"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['affiliation', 'weight', 'jockey', ..., 'country', 'state', 'name'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0F4knsmwpe7x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "bbfa7900-6573-4f64-d0ad-03dcb6af3ce2"
      },
      "source": [
        "import pandas as pd\n",
        "dd = pd.read_parquet('/data/data/raw/test_values.parquet')\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "values    ['Central Missouri', 'unattached', 'unattached...\n",
              "Name: 20368, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwZp5MJEqNjC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "2632aa83-e12d-474c-c3e9-86f8dedae2de"
      },
      "source": [
        "dd.iloc[0]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "values    ['Central Missouri', 'unattached', 'unattached...\n",
              "Name: 20368, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UffzKUW3qO3k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "b0e1012e-1234-428b-ff3d-5d539aa4e68b"
      },
      "source": [
        "val = dd.iloc[0]\n",
        "val = dd.iloc[4]\n",
        "val"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "values    ['AAF-McQuay Canada Inc.', 'AAF-McQuay Canada ...\n",
              "Name: 176253, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TtLnfHM2rFOu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "outputId": "8ad7fc71-fa5e-4f4c-9bb5-7af44ea29d8f"
      },
      "source": [
        "# proc = pd.read_parquet('/data/data/processed/X_val.parquet')\n",
        "train = pd.read_parquet('/data/data/processed/X_train.parquet')\n",
        "train.head()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>col_entropy</th>\n",
              "      <th>frac_unique</th>\n",
              "      <th>frac_numcells</th>\n",
              "      <th>frac_textcells</th>\n",
              "      <th>avg_num_cells</th>\n",
              "      <th>std_num_cells</th>\n",
              "      <th>avg_text_cells</th>\n",
              "      <th>std_text_cells</th>\n",
              "      <th>avg_spec_cells</th>\n",
              "      <th>std_spec_cells</th>\n",
              "      <th>avg_word_cells</th>\n",
              "      <th>std_word_cells</th>\n",
              "      <th>word_embedding_avg_0</th>\n",
              "      <th>word_embedding_avg_1</th>\n",
              "      <th>word_embedding_avg_2</th>\n",
              "      <th>word_embedding_avg_3</th>\n",
              "      <th>word_embedding_avg_4</th>\n",
              "      <th>word_embedding_avg_5</th>\n",
              "      <th>word_embedding_avg_6</th>\n",
              "      <th>word_embedding_avg_7</th>\n",
              "      <th>word_embedding_avg_8</th>\n",
              "      <th>word_embedding_avg_9</th>\n",
              "      <th>word_embedding_avg_10</th>\n",
              "      <th>word_embedding_avg_11</th>\n",
              "      <th>word_embedding_avg_12</th>\n",
              "      <th>word_embedding_avg_13</th>\n",
              "      <th>word_embedding_avg_14</th>\n",
              "      <th>word_embedding_avg_15</th>\n",
              "      <th>word_embedding_avg_16</th>\n",
              "      <th>word_embedding_avg_17</th>\n",
              "      <th>word_embedding_avg_18</th>\n",
              "      <th>word_embedding_avg_19</th>\n",
              "      <th>word_embedding_avg_20</th>\n",
              "      <th>word_embedding_avg_21</th>\n",
              "      <th>word_embedding_avg_22</th>\n",
              "      <th>word_embedding_avg_23</th>\n",
              "      <th>word_embedding_avg_24</th>\n",
              "      <th>word_embedding_avg_25</th>\n",
              "      <th>word_embedding_avg_26</th>\n",
              "      <th>word_embedding_avg_27</th>\n",
              "      <th>...</th>\n",
              "      <th>par_vec_360</th>\n",
              "      <th>par_vec_361</th>\n",
              "      <th>par_vec_362</th>\n",
              "      <th>par_vec_363</th>\n",
              "      <th>par_vec_364</th>\n",
              "      <th>par_vec_365</th>\n",
              "      <th>par_vec_366</th>\n",
              "      <th>par_vec_367</th>\n",
              "      <th>par_vec_368</th>\n",
              "      <th>par_vec_369</th>\n",
              "      <th>par_vec_370</th>\n",
              "      <th>par_vec_371</th>\n",
              "      <th>par_vec_372</th>\n",
              "      <th>par_vec_373</th>\n",
              "      <th>par_vec_374</th>\n",
              "      <th>par_vec_375</th>\n",
              "      <th>par_vec_376</th>\n",
              "      <th>par_vec_377</th>\n",
              "      <th>par_vec_378</th>\n",
              "      <th>par_vec_379</th>\n",
              "      <th>par_vec_380</th>\n",
              "      <th>par_vec_381</th>\n",
              "      <th>par_vec_382</th>\n",
              "      <th>par_vec_383</th>\n",
              "      <th>par_vec_384</th>\n",
              "      <th>par_vec_385</th>\n",
              "      <th>par_vec_386</th>\n",
              "      <th>par_vec_387</th>\n",
              "      <th>par_vec_388</th>\n",
              "      <th>par_vec_389</th>\n",
              "      <th>par_vec_390</th>\n",
              "      <th>par_vec_391</th>\n",
              "      <th>par_vec_392</th>\n",
              "      <th>par_vec_393</th>\n",
              "      <th>par_vec_394</th>\n",
              "      <th>par_vec_395</th>\n",
              "      <th>par_vec_396</th>\n",
              "      <th>par_vec_397</th>\n",
              "      <th>par_vec_398</th>\n",
              "      <th>par_vec_399</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.583978</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.000</td>\n",
              "      <td>2.481129</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.342</td>\n",
              "      <td>0.474380</td>\n",
              "      <td>-0.283456</td>\n",
              "      <td>0.124990</td>\n",
              "      <td>0.099320</td>\n",
              "      <td>0.497022</td>\n",
              "      <td>-0.054743</td>\n",
              "      <td>0.040936</td>\n",
              "      <td>-0.476492</td>\n",
              "      <td>-0.385870</td>\n",
              "      <td>1.034972</td>\n",
              "      <td>-0.289694</td>\n",
              "      <td>0.413443</td>\n",
              "      <td>0.112852</td>\n",
              "      <td>-0.460510</td>\n",
              "      <td>-0.078048</td>\n",
              "      <td>0.188527</td>\n",
              "      <td>0.433542</td>\n",
              "      <td>0.636364</td>\n",
              "      <td>-0.376642</td>\n",
              "      <td>-0.661308</td>\n",
              "      <td>0.068996</td>\n",
              "      <td>0.461603</td>\n",
              "      <td>-0.081154</td>\n",
              "      <td>-0.072629</td>\n",
              "      <td>-0.052363</td>\n",
              "      <td>0.041137</td>\n",
              "      <td>-1.538024</td>\n",
              "      <td>0.486184</td>\n",
              "      <td>-0.573166</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.020019</td>\n",
              "      <td>0.072385</td>\n",
              "      <td>0.039870</td>\n",
              "      <td>-0.031674</td>\n",
              "      <td>0.003470</td>\n",
              "      <td>0.001722</td>\n",
              "      <td>-0.073426</td>\n",
              "      <td>-0.061506</td>\n",
              "      <td>0.010845</td>\n",
              "      <td>-0.023536</td>\n",
              "      <td>0.001201</td>\n",
              "      <td>0.097940</td>\n",
              "      <td>-0.006483</td>\n",
              "      <td>0.030407</td>\n",
              "      <td>-0.025291</td>\n",
              "      <td>-0.013173</td>\n",
              "      <td>-0.030650</td>\n",
              "      <td>-0.063043</td>\n",
              "      <td>-0.002926</td>\n",
              "      <td>-0.018594</td>\n",
              "      <td>0.068280</td>\n",
              "      <td>-0.028801</td>\n",
              "      <td>-0.082349</td>\n",
              "      <td>-0.040430</td>\n",
              "      <td>-0.005640</td>\n",
              "      <td>0.093396</td>\n",
              "      <td>0.011162</td>\n",
              "      <td>0.047386</td>\n",
              "      <td>-0.033271</td>\n",
              "      <td>0.025629</td>\n",
              "      <td>-0.031846</td>\n",
              "      <td>-0.042225</td>\n",
              "      <td>-0.010270</td>\n",
              "      <td>0.016598</td>\n",
              "      <td>0.020357</td>\n",
              "      <td>-0.014771</td>\n",
              "      <td>0.003015</td>\n",
              "      <td>-0.045057</td>\n",
              "      <td>0.012795</td>\n",
              "      <td>-0.088069</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.709765</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>14.134</td>\n",
              "      <td>4.349718</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.388</td>\n",
              "      <td>0.790858</td>\n",
              "      <td>-0.124092</td>\n",
              "      <td>0.491391</td>\n",
              "      <td>-0.014307</td>\n",
              "      <td>-0.680386</td>\n",
              "      <td>0.603699</td>\n",
              "      <td>1.234906</td>\n",
              "      <td>-0.435464</td>\n",
              "      <td>-1.007747</td>\n",
              "      <td>0.978935</td>\n",
              "      <td>0.301690</td>\n",
              "      <td>0.292554</td>\n",
              "      <td>-0.282207</td>\n",
              "      <td>1.033366</td>\n",
              "      <td>0.632215</td>\n",
              "      <td>0.599736</td>\n",
              "      <td>-0.103853</td>\n",
              "      <td>-0.494512</td>\n",
              "      <td>0.522760</td>\n",
              "      <td>-0.298811</td>\n",
              "      <td>0.177962</td>\n",
              "      <td>-0.286621</td>\n",
              "      <td>0.659961</td>\n",
              "      <td>1.091843</td>\n",
              "      <td>1.357802</td>\n",
              "      <td>-0.318321</td>\n",
              "      <td>-0.868281</td>\n",
              "      <td>-0.276206</td>\n",
              "      <td>-0.533862</td>\n",
              "      <td>...</td>\n",
              "      <td>0.072373</td>\n",
              "      <td>0.009977</td>\n",
              "      <td>0.065009</td>\n",
              "      <td>-0.097925</td>\n",
              "      <td>-0.032315</td>\n",
              "      <td>-0.029577</td>\n",
              "      <td>-0.034491</td>\n",
              "      <td>-0.089080</td>\n",
              "      <td>0.106958</td>\n",
              "      <td>0.104118</td>\n",
              "      <td>-0.076484</td>\n",
              "      <td>0.055208</td>\n",
              "      <td>0.081074</td>\n",
              "      <td>-0.082153</td>\n",
              "      <td>-0.015922</td>\n",
              "      <td>-0.043516</td>\n",
              "      <td>0.024038</td>\n",
              "      <td>0.007980</td>\n",
              "      <td>0.088572</td>\n",
              "      <td>-0.082217</td>\n",
              "      <td>0.009583</td>\n",
              "      <td>0.044131</td>\n",
              "      <td>-0.110122</td>\n",
              "      <td>0.029886</td>\n",
              "      <td>-0.024660</td>\n",
              "      <td>0.023824</td>\n",
              "      <td>-0.043003</td>\n",
              "      <td>-0.110738</td>\n",
              "      <td>0.047209</td>\n",
              "      <td>0.112042</td>\n",
              "      <td>0.139149</td>\n",
              "      <td>-0.081729</td>\n",
              "      <td>-0.020694</td>\n",
              "      <td>-0.026501</td>\n",
              "      <td>0.024685</td>\n",
              "      <td>-0.045177</td>\n",
              "      <td>-0.019182</td>\n",
              "      <td>-0.000505</td>\n",
              "      <td>-0.026614</td>\n",
              "      <td>-0.139599</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2.683619</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.926</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>22.208</td>\n",
              "      <td>9.167919</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.846</td>\n",
              "      <td>1.635935</td>\n",
              "      <td>0.068061</td>\n",
              "      <td>0.425430</td>\n",
              "      <td>-0.240809</td>\n",
              "      <td>0.540709</td>\n",
              "      <td>-0.108423</td>\n",
              "      <td>-0.497726</td>\n",
              "      <td>-0.769251</td>\n",
              "      <td>-0.124409</td>\n",
              "      <td>0.088867</td>\n",
              "      <td>-0.159612</td>\n",
              "      <td>-0.029985</td>\n",
              "      <td>-0.434225</td>\n",
              "      <td>-0.004379</td>\n",
              "      <td>-0.440910</td>\n",
              "      <td>0.132148</td>\n",
              "      <td>0.010491</td>\n",
              "      <td>-0.028504</td>\n",
              "      <td>0.309783</td>\n",
              "      <td>-0.710389</td>\n",
              "      <td>-0.083151</td>\n",
              "      <td>0.011140</td>\n",
              "      <td>-0.142335</td>\n",
              "      <td>-0.375496</td>\n",
              "      <td>0.247376</td>\n",
              "      <td>-0.266335</td>\n",
              "      <td>-1.105923</td>\n",
              "      <td>-0.078606</td>\n",
              "      <td>0.063095</td>\n",
              "      <td>...</td>\n",
              "      <td>0.112560</td>\n",
              "      <td>0.029236</td>\n",
              "      <td>-0.055618</td>\n",
              "      <td>-0.012750</td>\n",
              "      <td>0.004203</td>\n",
              "      <td>0.093994</td>\n",
              "      <td>0.003757</td>\n",
              "      <td>-0.068219</td>\n",
              "      <td>0.133823</td>\n",
              "      <td>0.092441</td>\n",
              "      <td>-0.233094</td>\n",
              "      <td>0.101197</td>\n",
              "      <td>0.026390</td>\n",
              "      <td>-0.022992</td>\n",
              "      <td>-0.174207</td>\n",
              "      <td>-0.153883</td>\n",
              "      <td>-0.137559</td>\n",
              "      <td>-0.050265</td>\n",
              "      <td>0.008730</td>\n",
              "      <td>-0.042249</td>\n",
              "      <td>-0.006191</td>\n",
              "      <td>0.120627</td>\n",
              "      <td>-0.185379</td>\n",
              "      <td>0.134983</td>\n",
              "      <td>0.236772</td>\n",
              "      <td>0.125654</td>\n",
              "      <td>0.011504</td>\n",
              "      <td>0.016947</td>\n",
              "      <td>0.001509</td>\n",
              "      <td>0.011905</td>\n",
              "      <td>0.189321</td>\n",
              "      <td>-0.138547</td>\n",
              "      <td>0.008095</td>\n",
              "      <td>0.062142</td>\n",
              "      <td>-0.101572</td>\n",
              "      <td>-0.005824</td>\n",
              "      <td>0.188912</td>\n",
              "      <td>-0.106015</td>\n",
              "      <td>0.009559</td>\n",
              "      <td>-0.049206</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2.316898</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.635</td>\n",
              "      <td>4.773445</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.239</td>\n",
              "      <td>1.200783</td>\n",
              "      <td>0.144955</td>\n",
              "      <td>0.173220</td>\n",
              "      <td>-0.037712</td>\n",
              "      <td>0.238961</td>\n",
              "      <td>-0.027828</td>\n",
              "      <td>0.204401</td>\n",
              "      <td>-0.521712</td>\n",
              "      <td>-0.123938</td>\n",
              "      <td>-0.186294</td>\n",
              "      <td>0.486475</td>\n",
              "      <td>-0.069986</td>\n",
              "      <td>-0.002412</td>\n",
              "      <td>-0.221150</td>\n",
              "      <td>0.647568</td>\n",
              "      <td>0.543714</td>\n",
              "      <td>0.006660</td>\n",
              "      <td>0.071223</td>\n",
              "      <td>0.277305</td>\n",
              "      <td>-0.462491</td>\n",
              "      <td>0.175399</td>\n",
              "      <td>0.508501</td>\n",
              "      <td>0.668969</td>\n",
              "      <td>0.533301</td>\n",
              "      <td>0.667575</td>\n",
              "      <td>0.047458</td>\n",
              "      <td>-0.886161</td>\n",
              "      <td>-0.715458</td>\n",
              "      <td>0.036634</td>\n",
              "      <td>...</td>\n",
              "      <td>0.041294</td>\n",
              "      <td>0.055547</td>\n",
              "      <td>0.031728</td>\n",
              "      <td>-0.005034</td>\n",
              "      <td>-0.017738</td>\n",
              "      <td>-0.004500</td>\n",
              "      <td>-0.084307</td>\n",
              "      <td>-0.024763</td>\n",
              "      <td>0.022088</td>\n",
              "      <td>0.018773</td>\n",
              "      <td>-0.063726</td>\n",
              "      <td>-0.010242</td>\n",
              "      <td>0.060355</td>\n",
              "      <td>-0.037619</td>\n",
              "      <td>-0.017593</td>\n",
              "      <td>-0.029990</td>\n",
              "      <td>-0.014697</td>\n",
              "      <td>-0.058286</td>\n",
              "      <td>0.162420</td>\n",
              "      <td>-0.061752</td>\n",
              "      <td>0.065233</td>\n",
              "      <td>0.001361</td>\n",
              "      <td>-0.006204</td>\n",
              "      <td>-0.048913</td>\n",
              "      <td>0.043627</td>\n",
              "      <td>-0.044671</td>\n",
              "      <td>-0.040098</td>\n",
              "      <td>-0.004004</td>\n",
              "      <td>0.043567</td>\n",
              "      <td>0.038022</td>\n",
              "      <td>0.071699</td>\n",
              "      <td>-0.173113</td>\n",
              "      <td>-0.069093</td>\n",
              "      <td>0.114567</td>\n",
              "      <td>-0.072788</td>\n",
              "      <td>-0.069548</td>\n",
              "      <td>-0.042909</td>\n",
              "      <td>0.003029</td>\n",
              "      <td>-0.026417</td>\n",
              "      <td>-0.149617</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.368901</td>\n",
              "      <td>0.004</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.455</td>\n",
              "      <td>1.460813</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.212017</td>\n",
              "      <td>0.664065</td>\n",
              "      <td>-0.417174</td>\n",
              "      <td>0.009742</td>\n",
              "      <td>-0.448787</td>\n",
              "      <td>0.335266</td>\n",
              "      <td>-1.214907</td>\n",
              "      <td>0.165840</td>\n",
              "      <td>0.614950</td>\n",
              "      <td>-0.115111</td>\n",
              "      <td>0.206391</td>\n",
              "      <td>0.332510</td>\n",
              "      <td>0.554042</td>\n",
              "      <td>-0.236134</td>\n",
              "      <td>-0.569928</td>\n",
              "      <td>-0.815657</td>\n",
              "      <td>-0.067378</td>\n",
              "      <td>1.002771</td>\n",
              "      <td>-1.158014</td>\n",
              "      <td>0.662250</td>\n",
              "      <td>-1.460536</td>\n",
              "      <td>-0.471674</td>\n",
              "      <td>0.332657</td>\n",
              "      <td>-0.307691</td>\n",
              "      <td>0.098046</td>\n",
              "      <td>-0.850866</td>\n",
              "      <td>-0.019377</td>\n",
              "      <td>0.586299</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.026675</td>\n",
              "      <td>0.197411</td>\n",
              "      <td>-0.043677</td>\n",
              "      <td>0.088280</td>\n",
              "      <td>-0.037476</td>\n",
              "      <td>0.026579</td>\n",
              "      <td>-0.051500</td>\n",
              "      <td>-0.068392</td>\n",
              "      <td>0.109307</td>\n",
              "      <td>0.020582</td>\n",
              "      <td>-0.173511</td>\n",
              "      <td>-0.000909</td>\n",
              "      <td>0.035305</td>\n",
              "      <td>0.087658</td>\n",
              "      <td>-0.014833</td>\n",
              "      <td>-0.116036</td>\n",
              "      <td>0.118294</td>\n",
              "      <td>-0.077851</td>\n",
              "      <td>0.177408</td>\n",
              "      <td>0.016143</td>\n",
              "      <td>0.103092</td>\n",
              "      <td>-0.032521</td>\n",
              "      <td>0.023875</td>\n",
              "      <td>0.085944</td>\n",
              "      <td>0.221586</td>\n",
              "      <td>0.100009</td>\n",
              "      <td>-0.003755</td>\n",
              "      <td>-0.182700</td>\n",
              "      <td>0.008635</td>\n",
              "      <td>-0.024827</td>\n",
              "      <td>0.095804</td>\n",
              "      <td>-0.437798</td>\n",
              "      <td>-0.181881</td>\n",
              "      <td>-0.066755</td>\n",
              "      <td>-0.153834</td>\n",
              "      <td>-0.053369</td>\n",
              "      <td>0.055661</td>\n",
              "      <td>0.234615</td>\n",
              "      <td>-0.032799</td>\n",
              "      <td>-0.320608</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 1588 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   col_entropy  frac_unique  ...  par_vec_398  par_vec_399\n",
              "0     1.583978        0.003  ...     0.012795    -0.088069\n",
              "1     0.709765        0.002  ...    -0.026614    -0.139599\n",
              "2     2.683619        0.008  ...     0.009559    -0.049206\n",
              "3     2.316898        0.005  ...    -0.026417    -0.149617\n",
              "4     1.368901        0.004  ...    -0.032799    -0.320608\n",
              "\n",
              "[5 rows x 1588 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    }
  ]
}